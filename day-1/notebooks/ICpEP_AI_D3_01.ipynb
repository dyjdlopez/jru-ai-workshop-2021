{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICpEP AI D3-01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "15d34BQhLXhG18bGl3rJUfnI-xawU0g-G",
      "authorship_tag": "ABX9TyPyCMblw5RCJv3diVaFJPLX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyjdlopez/jru-ai-workshop-2021/blob/main/day-1/notebooks/ICpEP_AI_D3_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QkINMPYfU0M"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWch-Ydifbor"
      },
      "source": [
        "# Fundamentals of TensorFlow\n",
        "Copyright D.Lopez 2021 | All Rights reserved <br><br>\n",
        "\n",
        "[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.<br>\n",
        "TensorFlow provides several APIs that allow developers to develop a range of AI Apps from data estimation, computer vision, natural language processing, and even reinforcement learning.\n",
        "![image](https://camo.githubusercontent.com/c04e16c05de80dadbdc990884672fc941fdcbbfbb02b31dd48c248d010861426/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67)<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kfU9XdLfWLr"
      },
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N3HyWZEfqCs"
      },
      "source": [
        "## Part 1 Tensor Operations\n",
        "TensorFlow mainly operates using tensors (as its name suggests) so let’s try to use our current knowledge about tensors and apply it with our current platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om-K3XKVfvWM"
      },
      "source": [
        "### 1.1 NumPy and TensorFlow\n",
        "If you have enjoyed using matrices and tensors in NumPy, then performing tensor algebra in TensorFlow will just be a breeze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86TnJLlLCloY"
      },
      "source": [
        "np_tensor = np.array(3)\n",
        "tf_tensor = tf.constant(3)\n",
        "\n",
        "print(np_tensor)\n",
        "print(tf_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd4djNWnDlfM"
      },
      "source": [
        "np_mat = np.array([\n",
        "                   [1,2],\n",
        "                   [3,1]\n",
        "], dtype=float)\n",
        "tf_mat = tf.constant([\n",
        "                      [1,2],\n",
        "                      [3,1]\n",
        "], dtype=float)\n",
        "print(np_mat)\n",
        "print(tf_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev9M4QY7EFsi"
      },
      "source": [
        "type(tf_mat.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etWzPea6EriB"
      },
      "source": [
        "A = tf_mat\n",
        "B = tf.transpose(tf_mat)\n",
        "print(f\"Matrix A: \\n{A}\")\n",
        "print(f\"Matrix B: \\n{B}\")\n",
        "print(f\"Sum of Tensors: \\n{A+B}\")\n",
        "print(f\"Difference of Tensors: \\n{A-B}\")\n",
        "print(f\"Product of Tensors: \\n{A*B}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9REBXeY-FQR5"
      },
      "source": [
        "print(f\"Dot Product of Tensors: \\n{A@B}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMv1QzUNFgup"
      },
      "source": [
        "C = tf.reshape(A, [4,1])\n",
        "C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYdtdQ83f80j"
      },
      "source": [
        "## Part 2: Machine Learning Revisited\n",
        "As we recall, machine learning takes in data and a program to produce a rule or determine a pattern as opposed with traditional program that requires a pattern or rule together with the data to create a working system.\n",
        "\n",
        "Machine learning can be further classified into several cognitive paradigms:\n",
        "\n",
        "<b>Supervised learning</b>— is a type of machine learning that requires input data to have a feature and a label or the typical X data and y label format. Supervised learning requires its dataset to be:\n",
        "* Large (Volume)\n",
        "* Various\n",
        "* Valid\n",
        "\n",
        "<b>Unsupervised learning</b>—unlike input data from supervised learning, unsupervised learning data doesn't have labels. Unsupervised learning aims to find patterns in unexplored data. Typical applications of unsupervised learning include: dimension reduction and clustering.\n",
        "\n",
        "<b>Reinforcement learning</b>—the inputs for a reinforcement learning algorithm requires little to none data (in form of a dataset) to succeed in learning. Reinforcement learning aims to learn a rule, policy, or “way to do stuff” by determining whether its actions for a certain environment is rewarded or punished by its algorithm. The common uses of reinforcement learning included optimization.\n",
        "\n",
        "In the succeeding topics, we will be focusing on supervised learning using Deep Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxPHAkwTzANw"
      },
      "source": [
        "### 2.1 The Neuron (Again)\n",
        "![image](https://svitla.com/uploads/ckeditor/ArtificialNeuronModel_english.jpg)<br>\n",
        "\n",
        "Recalling our last discussion with the neuron, we found out that it is the basic unit of a neural network. The learning process of the neuron consists of a feed-forward propagation in which it takes in several inputs in which it is multiplied by some weights and fed into a transfer function and then subjected to an activation function; and a backward propagation routine where it computes for the loss and cost of a neuron and uses the error value to update the weights and repeating until it converges (or even diverge) to a certain period of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD_MSE7g4cHn"
      },
      "source": [
        "#Features\n",
        "X = np.arange(-1,5,dtype=float)\n",
        "def fx(x): return 2*x-1\n",
        "#Targets/Labels\n",
        "y = np.array(list(map(fx,X)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5wwqavN5uMf"
      },
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppb0E-3V6lGP"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.losses import MSE, MAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riIBCkPFL6B0"
      },
      "source": [
        "### Dense Layer\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "])\n",
        "lr=0.01\n",
        "model.compile(optimizer=SGD(learning_rate=lr),\n",
        "              loss=MSE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcIta0FR8CJ2"
      },
      "source": [
        "history1 = model.fit(X,y,epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRzQNuya97LX"
      },
      "source": [
        "plt.title('Loss Curve')\n",
        "plt.plot(history1.history['loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZvCIEC08J6G"
      },
      "source": [
        "model.predict([10.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH24DOVfgHxj"
      },
      "source": [
        "## Part 3: Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtjJgHe7gEt8"
      },
      "source": [
        "### 3.1 Multilayer Perceptron\n",
        "![image](https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png)\n",
        "\n",
        "As the name suggests, a multilayer perceptron (MLP) is a network of neurons or perceptrons arrange and connected horizontally and vertically. In this setup, neurons share knowledge along their respective layer and passes the activated values to the next layers to have a sense of \"deep\" learning. The concept of MLP gave rise to develop the new field of machine learning—Deep Learning, where we study about Artificial Neural Networks (ANN).\n",
        "\n",
        "An ANN consists of three parts:\n",
        "* Input layer\n",
        "* Hidden layer(s)\n",
        "* Output layer\n",
        "However, when counting the number of layers of a neural network we exclude the input layer since no learning is happening at the input layer or Layer 0 ($L0$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM5Ij87A4M-9"
      },
      "source": [
        "### Multilayer Perceptron\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(units=16,input_shape=[1]), #Hidden Layer\n",
        "  tf.keras.layers.Dense(units=1) #Output layer\n",
        "])\n",
        "\n",
        "lr=0.01\n",
        "model.compile(optimizer=SGD(learning_rate=lr),\n",
        "              loss=MSE)\n",
        "model.summary()\n",
        "history2=model.fit(X,y, epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGxoHxAXABZu"
      },
      "source": [
        "plt.title('Loss Curve')\n",
        "plt.plot(history1.history['loss'], label='Single Neuron')\n",
        "plt.plot(history2.history['loss'], label='MLP')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NA1htOXADp1"
      },
      "source": [
        "model.predict([10.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEFOKU_2gdgr"
      },
      "source": [
        "### 3.2 Activation Functions\n",
        "\n",
        "![image](https://www.researchgate.net/profile/Junxi_Feng/publication/335845675/figure/fig3/AS:804124836765699@1568729709680/Commonly-used-activation-functions-a-Sigmoid-b-Tanh-c-ReLU-and-d-LReLU.ppm)\n",
        "\n",
        "Back in our discussion about the neuron, we know that an activaiton function is quite crucial in getting the right values. Different activation functions are used for different objectives of learning. One factor to consider in choosing an activation function is the behavior of outputs per layer or the expected output of the machine learning task. Simply, identifying whether you are classifying data or predicting data could help which activation function to use.\n",
        "\n",
        "For a deeper discussion and implementation check out:\n",
        "* [Activation functions in TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
        "* [Activation functions in Keras](https://keras.io/api/layers/activations/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1UktcMkL8os"
      },
      "source": [
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.nn import sigmoid, tanh, softmax, relu, leaky_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGE7yR7sOtS5"
      },
      "source": [
        "inputs = tf.constant([\n",
        "                      [0.0,-1.2,2.4,32.0,-20.1]\n",
        "                      ])\n",
        "print(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2okgbGyLtto"
      },
      "source": [
        "### Sigmoid\n",
        "sigmoid_layer = Activation(sigmoid)\n",
        "sigmoid_layer(inputs).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OCh5YjL1Td"
      },
      "source": [
        "### Tanh\n",
        "tanh_layer = Activation(tanh)\n",
        "tanh_layer(inputs).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9lwERY0NMCk"
      },
      "source": [
        "### Softmax\n",
        "softmax_layer = Activation(softmax)\n",
        "softmax_layer(inputs).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61c_pc6TLvnQ"
      },
      "source": [
        "### ReLU\n",
        "relu_layer = Activation(relu)\n",
        "relu_layer(inputs).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FExN1MOuLxsM"
      },
      "source": [
        "### Leaky ReLU\n",
        "lrelu_layer = Activation(leaky_relu)\n",
        "lrelu_layer(inputs).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-8PIV6DgL-j"
      },
      "source": [
        "### 3.3 Computer Vision\n",
        "Using deep neural network to solve image processing problems leads to the developing computer vision solutions. The goal of computer vision is to mimic the visio-cognitive functions of the brain. The main activities being done in computer vision include image and video recognition systems. For almost a decade there are a lot of effort being made in improving image systems in which the following tasks were introduced:\n",
        "![image](https://3.bp.blogspot.com/-e-V_TvNbMSc/XJ7uRvmc4CI/AAAAAAAADPo/47Cg4DqI-g45qQEDRYuPwgaEiqqYDq2wACLcBGAs/s1600/cnn-extensions.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc093sCuDPkm"
      },
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1CWzEiHCPe3"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4oIE52FCZ_u"
      },
      "source": [
        "plt.imshow(training_images[0])\n",
        "training_images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUBDJz-YCTlt"
      },
      "source": [
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "373S4w6ICVvt"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Flatten(), \n",
        "                                    tf.keras.layers.Dense(64, activation=relu), \n",
        "                                    tf.keras.layers.Dense(10, activation=softmax)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48EODZC9DEN-"
      },
      "source": [
        "model.compile(optimizer=Adam(),\n",
        "              loss=sparse_categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "history3 = model.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw46Y5snDmwF"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Loss Curve')\n",
        "plt.plot(history3.history['loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Accuracy Curve')\n",
        "plt.plot(history3.history['accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY7gSg-8E5H_"
      },
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gh0Mvo4IryI"
      },
      "source": [
        "What if we add more layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTk7ac23fpZJ"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Flatten(), \n",
        "                                    tf.keras.layers.Dense(128, activation=relu), \n",
        "                                    tf.keras.layers.Dense(64, activation=relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=softmax)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2z34Q0qfj-j"
      },
      "source": [
        "model.compile(optimizer=Adam(),\n",
        "              loss=sparse_categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "history4 = model.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOt6EBEQG6j-"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Loss Curve')\n",
        "plt.plot(history4.history['loss'], label='2 Layers')\n",
        "plt.plot(history3.history['loss'], label='1 Layer')\n",
        "plt.legend()\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Accuracy Curve')\n",
        "plt.plot(history4.history['accuracy'], label='2 Layers')\n",
        "plt.plot(history3.history['accuracy'], label='1 Layer')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0mIY5GCIQfY"
      },
      "source": [
        "### 3.4 Convolutional Neural Networks\n",
        "Last week, we saw what convolutions do to images to get or localize some features. In deep learning, convolutions have also proven very useful in extracting information from images and learning from them.\n",
        "\n",
        "In 1998, [LeCunn et. al](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) introduced the implementation of convolutional neural networks for computer vision. But the first idea of using convolutions for visual cognition was [first conceptualized](https://medium.com/@gopalkalpande/biological-inspiration-of-convolutional-neural-network-cnn-9419668898ac) by Hubel and Wiesel in 1962, taking consideration of the processes visual cortex of the brain.\n",
        "\n",
        "![image](https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
        "\n",
        "You may notice some new terminologies in this section, let's break down the technicalities of CNNs.\n",
        "\n",
        "<b>Feature Maps</b>—feature maps are the learnt filters for convolutions. Recall that we need kernels in to perform convolutions, in CNNs we are trying to learn which are the appropriate values of kernels to get the feature that we want from the image.\n",
        "\n",
        "<b>Subsampling</b>—subsampling is a method of reducing the dimensions of an image or feature maps but retaining vital information. To achieve this, we tend to use pooling techniques such as maxima pooling or average pooling.\n",
        "\n",
        "<b>Full connections</b>—in the later layers of a CNN feature maps are flattened into a vector so we could run in through an MLP to determine the defining features of the the image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnlLXpInKPY8"
      },
      "source": [
        "mnist = mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdEpinDgJBok"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Conv2D(32, (3,3), \n",
        "                                 activation='relu', input_shape=(28,28,1)),\n",
        "          tf.keras.layers.MaxPooling2D((2,2)),\n",
        "          tf.keras.layers.Flatten(),\n",
        "          tf.keras.layers.Dense(128, activation='relu'),\n",
        "          tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=sparse_categorical_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history5= model.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QItNtyPJ-Pd"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Loss Curve')\n",
        "plt.plot(history5.history['loss'], label='CNN')\n",
        "plt.plot(history4.history['loss'], label='2 Layers')\n",
        "plt.plot(history3.history['loss'], label='1 Layer')\n",
        "plt.legend()\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Accuracy Curve')\n",
        "plt.plot(history5.history['accuracy'], label='CNN')\n",
        "plt.plot(history4.history['accuracy'], label='2 Layers')\n",
        "plt.plot(history3.history['accuracy'], label='1 Layer')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvcQZtLeNxCW"
      },
      "source": [
        "## Part 4: Custom Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeyfh8MwfzJT"
      },
      "source": [
        "### 4.1 Importing Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ6VjcOcF4hm"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls drive/MyDrive/ICpEP\\ AI\\ Workshop\\ Files/ds/\n",
        "data_dir = \"drive/MyDrive/ICpEP AI Workshop Files/ds/\"\n",
        "dir = os.listdir(data_dir)\n",
        "for folder in dir:\n",
        "  print(os.listdir(data_dir+folder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7q4JurSJSl-"
      },
      "source": [
        "dog_img = cv2.cvtColor(cv2.imread(data_dir+dir[0]+'/dog1.jpg'), \n",
        "                       cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(dog_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFtgaba_KQIe"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDMWuTkwLH23"
      },
      "source": [
        "TRAIN_DIR = data_dir+'train'\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "train_generator = train_datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                                     class_mode='binary',\n",
        "                                                     target_size=(100,100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn3JWqd4a46T"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_U24x3KbVtL"
      },
      "source": [
        "history6 = model.fit(train_generator,\n",
        "                              epochs=20,\n",
        "                              verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuLjvNcubiXr"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Loss Curve')\n",
        "plt.plot(history6.history['loss'])\n",
        "plt.legend()\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Accuracy Curve')\n",
        "plt.plot(history6.history['accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze6TmuPgZxjE"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "def upload_and_predict():\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "  \n",
        "    # predicting images\n",
        "    path = '/content/' + fn\n",
        "    img = image.load_img(path, target_size=(100, 100))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    images = np.vstack([x])\n",
        "    classes = model.predict(images)\n",
        "    print(classes[0])\n",
        "    if classes[0]>0.5:\n",
        "      print(fn + \" is a dog\")\n",
        "    else:\n",
        "      print(fn + \" is not a dog\")\n",
        "upload_and_predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_xdy2kuLvFZ"
      },
      "source": [
        "#### <i>Bias—Variance Tradeoff</i>\n",
        "![image](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F3335785%2F728627205a9dc976248c9f05a2baf464%2F1.png?generation=1600257419469907&alt=media)\n",
        "\n",
        "The bias—variance tradeoff is an essential topic to learn in debugging machine learning models. Overfitting and underfitting can be more understood if you know about biases and variance.\n",
        "\n",
        "<b>Bias</b>—the bias refers to the difference between the average prediction of the model and the groud truths. Models with high bias over-generalize the training data failing to \"learn\". This is refered to as <b>underfitting</b>.\n",
        "\n",
        "<b>Variance</b>—the variance of a model refers to the amount that the estimate of the target function will change if different training data was used. models with high variance is too \"specialized\" or overtrained on the dataset and fails to be flexible with data not in the training set. This is referred to as <b>overfitting</b>.\n",
        "\n",
        "<b><i>Remedies</b></i>\n",
        "\n",
        "To solve <b>underfitting</b> you might to:\n",
        "* Train longer, \n",
        "* Add features,\n",
        "* Choose more appropriate models,\n",
        "* Adding more layers (complexity) to the model, or\n",
        "* Choose a more appropriate loss or optimizer\n",
        "\n",
        "To solve <b>overfitting</b> you might to:\n",
        "* Train shorter, \n",
        "* Add more varieties or samples (generalizing), or\n",
        "* Apply regularization techniques such as batch normalization, data augmentation, dropout, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN34NuVbcwu4"
      },
      "source": [
        "### 4.2 Validation Sets\n",
        "To see the bias-variance tradeoff better in traning we need to have a validation set during training. The similarity between the test and validation set are that they are both data from the same dataset but not part of the training and they can be used to determine bias and variance. The difference is that validation sets are tested with the model during training while test sets are checked with the model post-training.\n",
        "\n",
        "There are also deployment sets that are considered another post-training dataset but are sampled from an actual area of deployment.\n",
        "\n",
        "For our example, we will be using [Image Data Generators](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator). Image data generators came from TensorFlow's image processing API. It allows us to pre-transform our dataset in-memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igfpkquncwDk"
      },
      "source": [
        "TRAIN_DIR = data_dir+'train'\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "train_generator = train_datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                                     class_mode='binary',\n",
        "                                                     target_size=(100,100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyvbHj-ec9Na"
      },
      "source": [
        "VALIDATION_DIR = data_dir+'test'\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "validaiton_generator = train_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                     class_mode='binary',\n",
        "                                                     target_size=(100,100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAZ0eMNBdC7r"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YkzlGGTeM7b"
      },
      "source": [
        "history7 = model.fit(train_generator,\n",
        "                              epochs=15,\n",
        "                              verbose=1,\n",
        "                     validation_data=validaiton_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEx1zRk_dKrb"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Loss Curve')\n",
        "plt.plot(history7.history['loss'])\n",
        "plt.plot(history7.history['val_loss'])\n",
        "plt.legend()\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Accuracy Curve')\n",
        "plt.plot(history7.history['accuracy'])\n",
        "plt.plot(history7.history['val_accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi65WESce5nj"
      },
      "source": [
        "upload_and_predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FOPTfZ8i6vz"
      },
      "source": [
        "### 4.3 Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r4wbgKqiamM"
      },
      "source": [
        "TRAIN_DIR = data_dir+'train'\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.,\n",
        "    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "train_generator = train_datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                                     class_mode='binary',\n",
        "                                                    batch_size=4,\n",
        "                                                     target_size=(100,100))\n",
        "\n",
        "VALIDATION_DIR = data_dir+'test'\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.,\n",
        "    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest'\n",
        "      )\n",
        "validaiton_generator = train_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                     class_mode='binary',\n",
        "                                                     batch_size=4,\n",
        "                                                     target_size=(100,100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyvQKJgWjaIr"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuscg8CYjhy8"
      },
      "source": [
        "history8 = model.fit(train_generator,\n",
        "                              epochs=15,\n",
        "                              verbose=1,\n",
        "                     validation_data=validaiton_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaYQSu-Tjvcj"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Loss Curve')\n",
        "plt.plot(history8.history['loss'])\n",
        "plt.plot(history8.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Accuracy Curve')\n",
        "plt.plot(history8.history['accuracy'])\n",
        "plt.plot(history8.history['val_accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GldXTooplMS7"
      },
      "source": [
        "upload_and_predict()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}